# -*- coding: utf-8 -*-
"""Copy of MLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yLPOCRGloPfdqF7ahjiWCvekHH4FiI9p

## **Import Libraries & Dataset**
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# scikit-learn imports
from sklearn import compose, impute, metrics, model_selection, pipeline, preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.metrics import (accuracy_score, auc, classification_report, confusion_matrix,
                             f1_score, precision_score, recall_score, roc_auc_score, RocCurveDisplay)
from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV, StratifiedKFold, train_test_split,
                                     cross_val_score)
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.svm import SVC, SVR
from sklearn.ensemble import (ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier,
                              RandomForestRegressor)
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB

# imblearn imports
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# scipy imports
from scipy.stats import chi2_contingency, randint

# Read the CSV file into a Pandas DataFrame
df = pd.read_csv('/content/Killed_and_Seriously_Injured.csv')

# Print the DataFrame
print(df)

#  basic exploration of df

# Print the first five rows of the DataFrame
print(df.head())

# Print the last five rows of the DataFrame
print(df.tail())

# Print the shape of the DataFrame
print(df.shape)

# Print the data types of each column in the DataFrame
print(df.dtypes)

# Print a summary of the DataFrame
print(df.describe())

# Print the number of missing values in each column
print(df.isnull().sum())

#Categorical columns
non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()
print(non_numeric_cols)

# Convert DATE column to datetime and remove timezone information
df['DATE'] = pd.to_datetime(df['DATE']).dt.tz_localize(None)

# Create a separate column for the year
df['YEAR'] = df['DATE'].dt.year

# Define a function to determine the season
def get_season(date):
    seasons = {
        'Winter': ((12, 1), (2, 28)),  # Adjusted to handle non-leap years
        'Spring': ((3, 1), (5, 31)),
        'Summer': ((6, 1), (8, 31)),
        'Fall': ((9, 1), (11, 30))
    }
    if date.month == 12 or date.month <= 2:
        season = 'Winter'
    else:
        for season, ((start_month, start_day), (end_month, end_day)) in seasons.items():
            if start_month <= date.month <= end_month:
                if start_month == date.month and date.day >= start_day:
                    return season
                elif end_month == date.month and date.day <= end_day:
                    return season
                elif start_month < date.month < end_month:
                    return season
    return 'Winter'  # Default case for December

# Define a function to determine if it is weekend
def is_weekend(date):
    # Check if it's Friday past 5pm or Saturday or Sunday before 4am Monday
    if (date.weekday() == 4 and date.hour >= 17) or \
       (date.weekday() == 5) or \
       (date.weekday() == 6) or \
       (date.weekday() == 0 and date.hour < 4):
        return 'Weekend'
    else:
        return 'Weekday'



# Apply the functions to create new columns
df['Season'] = df['DATE'].apply(get_season)
df['Weekend/Weekday'] = df['DATE'].apply(is_weekend)


print(df)

# Set display options to show all columns
pd.set_option('display.max_columns', None)


# Display the first 5 rows
df

#ACCLASS

#ACCLASS has ['Non-Fatal Injury' 'Fatal' 'Property Damage O' nan]; converting to Non-Fatal Injury' 'Fatal'

df['ACCLASS'].isna().sum()
#fill ACCLASS na value with most repeating value
df['ACCLASS'].fillna(df['ACCLASS'].mode()[0], inplace=True)

# convert 'Property Damage O' value in ACCLASS column to 'Non-Fatal Injury'
df['ACCLASS'] = df['ACCLASS'].replace('Property Damage O', 'Non-Fatal Injury')

unique_acclass = df['ACCLASS'].unique()
print(unique_acclass)


#----------------------------------------------------------------------------------

#VISIBILITY
# Fill missing values
df['VISIBILITY'].fillna('Unknown', inplace=True)

# Mapping for numerical representation
visibility_mapping = {
    'Clear': 0,
    'Snow': 1,
    'Rain': 1,
    'Fog, Mist, Smoke, Dust': 2,
    'Drifting Snow': 2,
    'Freezing Rain': 1,
    'Strong wind': 2,
    'Other': 3,
    'Unknown': 3
}

# Apply mapping
df['VISIBILITY'] = df['VISIBILITY'].map(visibility_mapping)

# Check for NaN values after mapping
if df['VISIBILITY'].isnull().sum() > 0:
    print("VISIBILITY column has NaN values after mapping.")

#----------------------------------------------------------------------------------

#LIGHT

# Fill missing values
df['LIGHT'].fillna('Unknown', inplace=True)

# Mapping for numerical representation
light_mapping = {
    'Dark': 0,
    'Dark, artificial': 0,
    'Dusk': 1,
    'Dusk, artificial': 1,
    'Dawn': 1,
    'Dawn, artificial': 1,
    'Daylight': 2,
    'Daylight, artificial': 2,
    'Other': 3,
    'Unknown': 3
}

# Apply mapping
df['LIGHT'] = df['LIGHT'].map(light_mapping)

# Check for NaN values after mapping
if df['LIGHT'].isnull().sum() > 0:
    print("LIGHT column has NaN values after mapping.")

# Filter out unwanted values
df = df[df['LIGHT'] != 3]

#----------------------------------------------------------------------------------
#RDSFCOND

# Fill missing values
df['RDSFCOND'].fillna('Unknown', inplace=True)

# Mapping for numerical representation
rdsfcond_mapping = {
    'Wet': 0,
    'Slush': 0,
    'Spilled liquid': 0,
    'Ice': 1,
    'Loose Snow': 1,
    'Packed Snow': 1,
    'Dry': 2,
    'Loose Sand or Gravel': 3,
    'Other': 4,
    'Unknown': 4
}

# Apply mapping
df['RDSFCOND'] = df['RDSFCOND'].map(rdsfcond_mapping)

# Check for NaN values after mapping
if df['RDSFCOND'].isnull().sum() > 0:
    print("RDSFCOND column has NaN values after mapping.")

# Filter out unwanted values
df = df[(df['RDSFCOND'] != 3) & (df['RDSFCOND'] != 4)]

#----------------------------------------------------------------------------------

#DRIVACT
drivact_mapping = {
    'Driving Properly': 'Proper Driving',
    'Failed to Yield Right of Way': 'Failure to Yield',
    'Lost control': 'Loss of Control',
    'Improper Turn': 'Improper Maneuvers',
    'Improper Lane Change': 'Improper Maneuvers',
    'Improper Passing': 'Improper Maneuvers',
    'Wrong Way on One Way Road': 'Improper Maneuvers',
    'Disobeyed Traffic Control': 'Disobedience of Traffic Control',
    'Other': 'Disobedience of Traffic Control',
    'Exceeding Speed Limit': 'Speed-Related',
    'Speed too Fast For Condition': 'Speed-Related',
    'Speed too Slow': 'Speed-Related',
    'Following too Close': 'Following Too Close'
}

# Apply the mapping
df['DRIVACT_GROUPED'] = df['DRIVACT'].map(drivact_mapping)

# Check for NaN values after mapping
if df['DRIVACT_GROUPED'].isnull().sum() > 0:
    print("DRIVACT_GROUPED column has NaN values after mapping.")

# Check the grouped value counts
print(df['DRIVACT_GROUPED'].value_counts())

# Drop original column
df.drop(columns=['DRIVACT'], inplace=True)

#----------------------------------------------------------------------------------

#INVAGE

# Define the mapping for grouped age categories
invage_mapping = {
    'unknown': 'Unknown',
    '0 to 4': 'Young',
    '5 to 9': 'Young',
    '10 to 14': 'Young',
    '15 to 19': 'Young',
    '20 to 24': 'Young Adults',
    '25 to 29': 'Young Adults',
    '30 to 34': 'Young Adults',
    '35 to 39': 'Middle-Aged',
    '40 to 44': 'Middle-Aged',
    '45 to 49': 'Middle-Aged',
    '50 to 54': 'Middle-Aged',
    '55 to 59': 'Older Adults',
    '60 to 64': 'Older Adults',
    '65 to 69': 'Older Adults',
    '70 to 74': 'Older Adults',
    '75 to 79': 'Older Adults',
    '80 to 84': 'Older Adults',
    '85 to 89': 'Older Adults',
    '90 to 94': 'Older Adults',
    'Over 95': 'Older Adults'
}

# Apply the mapping
df['INVAGE_GROUPED'] = df['INVAGE'].map(invage_mapping)

# Fill NaN values with mode
df['INVAGE_GROUPED'].fillna(df['INVAGE_GROUPED'].mode()[0], inplace=True)

# Drop original column
df.drop(columns=['INVAGE'], inplace=True)

#----------------------------------------------------------------------------------
#ROAD_CLASS

# Define mapping for Primary Road
road_class_mapping = {
    'Major Arterial': 1,
    'Minor Arterial': 1,
    'Expressway': 1,
    'Collector': 0,
    'Local': 0,
    'Expressway Ramp': 0,
    'Other': 0,
    'Laneway': 0,
    'Pending': 0,
    'Major Shoreline': 0
}

df['Primary_Road'] = df['ROAD_CLASS'].map(road_class_mapping)

# Drop rows where mapping failed
df.dropna(subset=['Primary_Road'], inplace=True)

# Drop original column
df = df.drop('ROAD_CLASS', axis=1)

#----------------------------------------------------------------------------------
#DRIVCOND
drivcond_mapping = {
    'Normal': 'Normal',
    'Inattentive': 'Distracted/Inattentive',
    'Unknown': 'Unknown/Other',
    'Medical or Physical Disability': 'Medical/Physical Issues',
    'Had Been Drinking': 'Impaired',
    'Ability Impaired, Alcohol Over .08': 'Impaired',
    'Ability Impaired, Alcohol': 'Impaired',
    'Other': 'Unknown/Other',
    'Fatigue': 'Distracted/Inattentive',
    'Ability Impaired, Drugs': 'Impaired'
}

# Apply mapping
df['DRIVCOND_GROUPED'] = df['DRIVCOND'].map(drivcond_mapping)
df = df.drop('DRIVCOND', axis=1)

#----------------------------------------------------------------------------------
#VEHTYPE

# Define the mapping
vehicle_type_mapping = {
    'Automobile, Station Wagon': 'Automobiles and Light Vehicles',
    'Pick Up Truck': 'Automobiles and Light Vehicles',
    'Passenger Van': 'Automobiles and Light Vehicles',
    'Delivery Van': 'Automobiles and Light Vehicles',
    'Taxi': 'Automobiles and Light Vehicles',
    'Bicycle': 'Bicycles and Motorcycles',
    'Motorcycle': 'Bicycles and Motorcycles',
    'Moped': 'Bicycles and Motorcycles',
    'Municipal Transit Bus (TTC)': 'Public Transit and Buses',
    'Street Car': 'Public Transit and Buses',
    'Bus (Other) (Go Bus, Gray Coa)': 'Public Transit and Buses',
    'Intercity Bus': 'Public Transit and Buses',
    'School Bus': 'Public Transit and Buses',
    'Truck - Open': 'Trucks and Heavy Vehicles',
    'Truck - Closed (Blazer, etc)': 'Trucks and Heavy Vehicles',
    'Truck - Dump': 'Trucks and Heavy Vehicles',
    'Truck-Tractor': 'Trucks and Heavy Vehicles',
    'Truck - Tank': 'Trucks and Heavy Vehicles',
    'Tow Truck': 'Trucks and Heavy Vehicles',
    'Truck - Car Carrier': 'Trucks and Heavy Vehicles',
    'Truck (other)': 'Trucks and Heavy Vehicles',
    'Construction Equipment': 'Trucks and Heavy Vehicles',
    'Police Vehicle': 'Emergency Vehicles',
    'Fire Vehicle': 'Emergency Vehicles',
    'Other Emergency Vehicle': 'Emergency Vehicles',
    'Ambulance': 'Emergency Vehicles',
    'Other': 'Others',
    'Unknown': 'Others',
    'Off Road - 2 Wheels': 'Others',
    'Off Road - 4 Wheels': 'Others',
    'Rickshaw': 'Others'
}


# Apply the mapping to create a new column
df['VEHTYPE_GROUPED'] = df['VEHTYPE'].map(vehicle_type_mapping)
df['VEHTYPE_GROUPED'] = df['VEHTYPE_GROUPED'].fillna('Unknown')

# Drop the original 'VEHTYPE' column
df = df.drop(columns=['VEHTYPE'])

#----------------------------------------------------------------------------------
#TRAFFCTL

# Fill NaN values with 'Unknown'
df['TRAFFCTL'] = df['TRAFFCTL'].fillna('Unknown')

# Define the mapping
traffic_control_mapping = {
    'No Control': 'No Control',
    'Traffic Signal': 'Traffic Signals and Signs',
    'Stop Sign': 'Traffic Signals and Signs',
    'Yield Sign': 'Traffic Signals and Signs',
    'Pedestrian Crossover': 'Traffic Signals and Signs',
    'Traffic Controller': 'Traffic Signals and Signs',
    'Streetcar (Stop for)': 'Controlled Intersections',
    'Traffic Gate': 'Controlled Intersections',
    'School Guard': 'Controlled Intersections',
    'Police Control': 'Controlled Intersections',
    'Unknown': 'Other'  # Handle unknowns
}

# Apply the mapping to create a new column
df['TRAFFCTL_GROUPED'] = df['TRAFFCTL'].map(traffic_control_mapping)

df = df.drop(columns=['TRAFFCTL'])

#----------------------------------------------------------------------------------
# INVTYPE -> Driver

df.rename(columns={'INVTYPE': 'Driver'}, inplace=True)

# Define the mapping for Driver column
driver_mapping = {
    'Driver': 1,
    'Motorcycle Driver': 1,
    'Truck Driver': 1,
    'Vehicle Owner': 1,
    'Other': 0,  # Assuming all other categories are not drivers
    'Pedestrian': 0,
    'Cyclist': 1,
    'Passenger': 0,
    'Other Property Owner': 1,
    'Motorcycle Passenger': 0,
    'Wheelchair': 0,
    'Driver - Not Hit': 0,
    'In-Line Skater': 0,
    'Cyclist Passenger': 0,
    'Trailer Owner': 1,
    'Pedestrian - Not Hit': 0,
    'Witness': 0,
    'Moped Passenger': 0,
    'Moped Driver': 1
}

# Apply the mapping to the 'Driver' column
df['Driver'] = df['Driver'].map(driver_mapping)

# fill null or nan values with 0 and convert Yes to 1.

columns_to_convert = ['PEDESTRIAN', 'CYCLIST', 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK',
                     'TRSN_CITY_VEH', 'EMERG_VEH', 'PASSENGER', 'SPEEDING', 'AG_DRIV',
                     'REDLIGHT', 'ALCOHOL', 'DISABILITY']

for col in columns_to_convert:
  df[col] = df[col].fillna(0)
  df[col] = df[col].replace('Yes', 1)

# Convert 'Weekend/Weekday' to 0/1
df['Weekend'] = df['Weekend/Weekday'].apply(lambda x: 1 if x == 'Weekend' else 0)
df = df.drop(columns=['Weekend/Weekday'])


# ACCLASS -> Fatal (0/1)

df['ACCLASS'] = df['ACCLASS'].apply(lambda x: 1 if x == 'Fatal' else 0)
df.rename(columns={'ACCLASS': 'Fatal'}, inplace=True)

#  ACCLOC having values 'At Intersection', 'Intersection Related' encode to 1, rest 0, missing values 2 and rename ACCLOC to At Intersection
def encode_accloc(value):
  if value in ['At Intersection', 'Intersection Related']:
    return 1
  elif pd.isna(value):
    return 2
  else:
    return 0

#ACCLOC
df['At_Intersection'] = df['ACCLOC'].apply(encode_accloc)
df.drop('ACCLOC', axis=1, inplace=True)

#drop columns with missing values more than 60%

# Calculate the threshold for missing values (60%)
threshold = len(df) * 0.6

# Identify columns with missing values exceeding the threshold
columns_to_drop = df.columns[df.isnull().sum() > threshold]

# Drop the identified columns
df = df.drop(columns=columns_to_drop)

# drop

# Drop specified columns
columns_to_drop = ['STREET1', 'STREET2', 'ACCNUM', 'DATE', 'OBJECTID', 'INDEX_', 'X', 'Y',
                   'INITDIR', 'IMPACTYPE', 'INVTYPE', 'HOOD_158','HOOD_ID','HOOD_140', 'MANOEUVER',
                   'NEIGHBOURHOOD_158', 'INTERSECTION', 'NEIGHBOURHOOD_140', 'DIVISION',
                   'TIME', 'LATITUDE', 'YEAR', 'LONGITUDE', 'Season', 'VEHTYPE', 'INJURY']
df = df.drop(columns=columns_to_drop, errors='ignore')  # Use errors='ignore' to avoid errors if a column doesn't exist

"""# Data Modelling Begins"""

# data columns type

df.info()

df.dtypes

df['VEHTYPE_GROUPED'] = df['VEHTYPE_GROUPED'].fillna('Unknown')

df['VEHTYPE_GROUPED'].value_counts()
#df['VEHTYPE_GROUPED'].isna().sum()

#df['DRIVACT_GROUPED'].value_counts()
#df['DRIVACT_GROUPED'].isna().sum()

df

import pandas as pd
import time
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.exceptions import ConvergenceWarning
import warnings

warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Define your features
features_columns_categorical = ['DISTRICT', 'DRIVACT_GROUPED', 'INVAGE_GROUPED', 'DRIVCOND_GROUPED', 'VEHTYPE_GROUPED', 'TRAFFCTL_GROUPED']
features_columns_numerical = ['VISIBILITY', 'LIGHT', 'RDSFCOND', 'Driver', 'PEDESTRIAN', 'CYCLIST', 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH', 'PASSENGER', 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY', 'Primary_Road', 'Weekend', 'At Intersection']

def get_pipeline_x_y(df, test_size=0.20):
    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = df.select_dtypes(include=['object', 'category']).columns

    # Remove the target variable from features
    if 'Fatal' in numeric_features:
        numeric_features = numeric_features.drop('Fatal')
    if 'Fatal' in categorical_features:
        categorical_features = categorical_features.drop('Fatal')

    print(f"Numeric features: {numeric_features}")
    print(f"Categorical features: {categorical_features}")

    # Define the pipelines
    categorical_pipeline = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    numeric_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('scaler', StandardScaler()),
    ])

    # Full pipeline - Column Transformer
    full_pipeline_transformer = ColumnTransformer([
        ("num", numeric_pipeline, numeric_features),
        ("cat", categorical_pipeline, categorical_features),
    ])

    # Extract features and target variable
    X_group = df[numeric_features.tolist() + categorical_features.tolist()]
    Y_group = df['Fatal']

    # Divide data into train/test
    X_train, X_test, y_train, y_test = train_test_split(X_group, Y_group, test_size=test_size, random_state=42)

    return full_pipeline_transformer, X_group, Y_group, X_train, X_test, y_train, y_test

def train_and_evaluate_models(X_train, X_test, y_train, y_test):
    models = {
        'Logistic Regression': LogisticRegression(class_weight='balanced', random_state=42, max_iter=5000, solver='saga'),
        'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=42),
        'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42),
        'SVM': SVC(class_weight='balanced', random_state=42, probability=True)
    }

    results = {}
    plt.figure(figsize=(10, 8))

    for name, model in models.items():
        print(f"\nTraining {name}...")
        start_time = time.time()

        pipeline = ImbPipeline([
            ('smote', SMOTE(random_state=42)),
            ('model', model)
        ])

        pipeline.fit(X_train, y_train)

        y_pred = pipeline.predict(X_test)
        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        auc_score = roc_auc_score(y_test, y_pred_proba)

        print("Performing cross-validation...")
        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='roc_auc')

        results[name] = {
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1 Score': f1,
            'AUC': auc_score,
            'CV AUC': cv_scores.mean()
        }

        end_time = time.time()
        print(f"{name} completed in {end_time - start_time:.2f} seconds")

        # Print interim results
        print(f"Interim results for {name}:")
        for metric, value in results[name].items():
            print(f"{metric}: {value:.4f}")

        # Plot ROC curve
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        plt.plot(fpr, tpr, label=f"{name} (AUC = {auc_score:.2f})")

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve for All Models')
    plt.legend(loc="lower right")
    plt.show()

    return results, models

# Assuming df is your DataFrame
print("Preparing data...")
full_pipeline_transformer, X_group, Y_group, X_train, X_test, y_train, y_test = get_pipeline_x_y(df)

print("Transforming data...")
X_train_transformed = full_pipeline_transformer.fit_transform(X_train)
X_test_transformed = full_pipeline_transformer.transform(X_test)

# Convert to numpy arrays if they aren't already
X_train_transformed = np.array(X_train_transformed)
X_test_transformed = np.array(X_test_transformed)
y_train = np.array(y_train)
y_test = np.array(y_test)

print("Training and evaluating models...")
results, models = train_and_evaluate_models(X_train_transformed, X_test_transformed, y_train, y_test)

# Print final results
for model, metrics in results.items():
    print(f"\n{model}:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")

# Find the best model based on AUC
best_model_name = max(results, key=lambda x: results[x]['AUC'])
best_model_object = models[best_model_name]  # Get the actual model object
print(f"\nBest model based on AUC: {best_model_name}")

# Save the best model using joblib
joblib.dump(best_model_object, 'best_model.pkl')

# Save the full pipeline transformer using joblib
joblib.dump(full_pipeline_transformer, 'full_pipeline_transformer.pkl')

# Print the first 10 predictions
y_pred = best_model_object.predict(X_test_transformed)
print("\nFirst 10 Predictions:")
print(y_pred[:10])

# Print classification counts
num_survived = np.sum(y_pred == 0)
num_killed = np.sum(y_pred == 1)
print(f"\nClassification Counts:\nSurvived: {num_survived}\nKilled: {num_killed}")

